---
title: 框架识别&泄露提取&API接口枚举&FUZZ爬虫
published: 2025-10-08
description: Web应用框架识别、fuzz爆破JS案例，packer-fuzzer检测。
tags: [Web,信息打点]
category: 网络安全
draft: false
---

## 知识补充

1. 什么是JS渗透测试？

在 Javascript 中也存在变量和函数，当存在**可控变量及函数调用**即可参数漏洞 JS 开发的 WEB 应用和 PHP，JAVA,NET 等区别在于**即没有源代码，也可以通过浏览器的查看源代码获取真实的点。** **获取 URL，获取 JS 敏感信息，获取代码传参**等，所以相当于 JS 开发的 WEB 应用属于白盒测试（默认有源码参考），一般会在 JS 中寻找更多的 URL 地址，在JS 代码逻辑（加密算法，APIkey 配置，验证逻辑等）进行后期安全测试。

### 前提：Web 应用可以采用后端或前端语言开发

- 后端语言：php java python .NET 浏览器端看不到真实的源代码

- 前端语言：JavaScript(JS)和 JS 框架 浏览器端看到真实的源代码

  例子：

  zblog：核心功能采用PHP语言去传输接受。

  vue.js：核心功能采用框架语法（JS）传输接受。

## JS 前端架构-开发框架分析

1. JS安全问题
   - 源码泄漏。
   - 未授权访问=JS里面分析更多的URL访问确定接口路径。
   - 敏感key泄漏=JS文件中可能配置了接口信息（云应用，短信，邮件，数据库等）。
   - API接口安全=（代码中加密提交参数传递，更多的URL路径）。

2. 流行的JS框架

   Vue NodeJS jQuery Angular等。

3. 如何判定JS开发应用？
   - 插件wappalyzer。
   - 源程序代码简短。
   - 引入多个js文件。
   - 一般有/static/js/app.js等顺序的js文件。
   - 一般cookie中有connect.sid。

4. 如何获取更多的JS文件？
   - 手工模式：分析时间长，但是精准。
   - 半自动模式：居中。
   - 全自动模式：分析时间短，精准度低。

5. 快速获取价值信息—JS代码中常见的重要标签和函数

   - src= ——链接地址
   - path=     ——路径
   - method:"get"     ——请求方法
   - http.get("
   - method:"post"
   - http.post("
   - $.ajax
   - http://service.httppos
   - http://service.httpget

   以上搜索都是为了寻找链接地址和路径，目的就是为了得到URL上的其他访问，就可以尝试去访问这些地址来看是什么东西，有没有对渗透是有帮助的。

## JS 前端架构-打包器分析

1. 前端架构-手工搜索分析

   浏览器全局搜索分析

2. 前端架构-半自动bp分析

   自带功能：Target->sitemap->Engagement tools->Find scripts

   官方插件：JS Link Finder & JS Miner

   第三方插件：HaE & Unexpected_information

   插件加载器：jython-standalone-2.7.2

   - Unexpected_information：https://github.com/ScriptKid-Beta/Unexpected_information。用来标记请求包中的一些敏感信息、JS接口和一些特殊字段，防止我们疏忽了一些数据包，使用它可能会有意外的收获信息。
   - HaE：https://github.com/gh0stkey/HaE ； https://raw.githubusercontent.com/gh0stkey/HaE/gh-pages/Config.yml。基于BurpSuite插件JavaAPI开发的请求高亮标记与信息提取的辅助型插件。该插件可以通过自定义正则的方式匹配响应报文或请求报文，可以自行决定符合该自定义正则匹配的相应请求是否需要高亮标记、信息提取。

## JS前端架构-自动化项目分析

1. **URLFinder**

   从表现中JS中提取URL或者敏感数据

   https://github.com/pingc0y/URLFinder

   一款用于快速提取检测页面中JS与URL的工具

   功能类似于JSFinder，但JSFinder好久没更新了

2. JSINFO-SCAN

   从表现中JS中提取URL或者敏感数据

   https://github.com/p1g3/JSINFO-SCAN

   递归爬取域名(netloc/domain)，以及递归从JS中获取信息的工具

3. FindSomething

   从表现中JS中提取URL或者敏感数据

   https://github.com/momosecurity/FindSomething

   该工具是用于快速在网页的html源码或js代码中提取一些有趣的信息的浏览器插件，

   包括请求的资源、接口的url，请求的ip和域名，泄漏的证件号、手机号、邮箱等信息。

4. Packer-Fuzzer

   针对JS框架开发打包器Webpack检测

   https://github.com/rtcatc/Packer-Fuzzer

   一款针对Webpack等前端打包工具所构造的网站进行快速、高效安全检测的扫描工具，只能检测webpack。

## 分析案例1

利用以上提到的一些工具以及御剑进行扫描，看看结果如何。

通过wappalyzer可以看到，该站的开发语言，以及框架，下图所示：

![](https://cdn.jsdelivr.net/gh/PWN022/0x00@main/NetSecurity/My_screenshot/14-05.png)

通过Findeverything可以看到该站的一些敏感信息、路径、访问地址等信息，如下图：

![](https://cdn.jsdelivr.net/gh/PWN022/0x00@main/NetSecurity/My_screenshot/14-04.png)

此时再使用URLFinder进行扫描，看看是什么样的结果。

这里贴一下URLFinder中的一些参数：

```shell
-a  自定义user-agent请求头  
-b  自定义baseurl路径  
-c  请求添加cookie  
-d  指定获取的域名,支持正则表达式
-f  批量url抓取,需指定url文本路径  
-ff 与-f区别：全部抓取的数据,视为同一个url的结果来处理（只打印一份结果 | 只会输出一份结果） 
-h  帮助信息   
-i  加载yaml配置文件,可自定义请求头、抓取规则等（不存在时,会在当前目录创建一个默认yaml配置文件）  
-m  抓取模式：
        1  正常抓取（默认）
        2  深入抓取 （URL深入一层 JS深入三层 防止抓偏）
        3  安全深入抓取（过滤delete,remove等敏感路由） 
-max 最大抓取数
-o  结果导出到csv、json、html文件,需指定导出文件目录（.代表当前目录）
-s  显示指定状态码,all为显示全部  
-t  设置线程数（默认50）
-time 设置超时时间（默认5,单位秒）
-u  目标URL  
-x  设置代理,格式: http://username:password@127.0.0.1:8877
-z  提取所有目录对404链接进行fuzz(只对主域名下的链接生效,需要与 -s 一起使用）  
        1  目录递减fuzz  
        2  2级目录组合fuzz
        3  3级目录组合fuzz（适合少量链接使用）
```

URLFiner中的扫描结果如下：

![](https://cdn.jsdelivr.net/gh/PWN022/0x00@main/NetSecurity/My_screenshot/14-06.png)

可以使用插件+软件搭配不同程度的扫描，来获取更多的结果，此时也就有更多的机会来对网站进行想要进行的操作。

## JS前端架构-提取&FUZZ

ffuf-FUZZ 爆破找到更多的js文件分析更多的信息。

https://github.com/ffuf/ffuf

https://wordlists.assetnote.io ——字典列表，可以根据不同类型开发的网站进行相应的字典选择。

功能强大的模糊化工具，用它来FUZZ模糊化js文件。

FUZZ的参数如下：

```shell
![14-07](E:\Files\PIC\14-07.png)HTTP OPTIONS:
  -H                  添加请求头，格式 "名称: 值"，可多次使用。
  -X                  指定 HTTP 方法（GET/POST/PUT…）。
  -b                  携带 Cookie，格式 "NAME1=VALUE1; NAME2=VALUE2"。.
  -cc                 客户端证书（需同时给出 -ck）。
  -ck                 客户端私钥（需同时给出 -cc）。
  -d                  POST 数据体。
  -http2              使用 HTTP/2 协议（默认否）。
  -ignore-body        不下载响应正文（默认否）。
  -r                  跟随 30x 重定向（默认否）。
  -raw                不对 URI 进行编码（默认否）。
  -recursion          递归扫描；仅支持 FUZZ 关键词，且 URL 必须以 FUZZ 结尾（默认否）。
  -recursion-depth    最大递归深度（默认 0，无限）。
  -recursion-strategy 递归策略：default（仅跟重定向）或 greedy（匹配到的都递归，默认 default）。
  -replay-proxy       将匹配到的请求再重放给指定代理。
  -sni                指定 TLS 的 SNI（不支持 FUZZ 关键词）。
  -timeout            单次请求超时秒数（默认 10）。
  -u                  目标 URL。
  -x                  代理地址，格式 http://127.0.0.1:8080 或 socks5://127.0.0.1:8080。

GENERAL OPTIONS:
  -V                  Show version information. (default: false)
  -ac                 Automatically calibrate filtering options (default: false)
  -acc                Custom auto-calibration string. Can be used multiple times. Implies -ac
  -ach                Per host autocalibration (default: false)
  -ack                Autocalibration keyword (default: FUZZ)
  -acs                Custom auto-calibration strategies. Can be used multiple times. Implies -ac
  -c                  Colorize output. (default: false)
  -config             Load configuration from a file
  -json               JSON output, printing newline-delimited JSON records (default: false)
  -maxtime            Maximum running time in seconds for entire process. (default: 0)
  -maxtime-job        Maximum running time in seconds per job. (default: 0)
  -noninteractive     Disable the interactive console functionality (default: false)
  -p                  Seconds of `delay` between requests, or a range of random delay. For example "0.1" or "0.1-2.0"
  -rate               Rate of requests per second (default: 0)
  -s                  Do not print additional information (silent mode) (default: false)
  -sa                 Stop on all error cases. Implies -sf and -se. (default: false)
  -scraperfile        Custom scraper file path
  -scrapers           Active scraper groups (default: all)
  -se                 Stop on spurious errors (default: false)
  -search             Search for a FFUFHASH payload from ffuf history
  -sf                 Stop when > 95% of responses return 403 Forbidden (default: false)
  -t                  Number of concurrent threads. (default: 40)
  -v                  Verbose output, printing full URL and redirect location (if any) with the results. (default: false)

MATCHER OPTIONS:
  -mc                 Match HTTP status codes, or "all" for everything. (default: 200-299,301,302,307,401,403,405,500)
  -ml                 Match amount of lines in response
  -mmode              Matcher set operator. Either of: and, or (default: or)
  -mr                 Match regexp
  -ms                 Match HTTP response size
  -mt                 Match how many milliseconds to the first response byte, either greater or less than. EG: >100 or <100
  -mw                 Match amount of words in response

FILTER OPTIONS:
  -fc                 Filter HTTP status codes from response. Comma separated list of codes and ranges
  -fl                 Filter by amount of lines in response. Comma separated list of line counts and ranges
  -fmode              Filter set operator. Either of: and, or (default: or)
  -fr                 Filter regexp
  -fs                 Filter HTTP response size. Comma separated list of sizes and ranges
  -ft                 Filter by number of milliseconds to the first response byte, either greater or less than. EG: >100 or <100
  -fw                 Filter by amount of words in response. Comma separated list of word counts and ranges

INPUT OPTIONS:
  -D                  DirSearch wordlist compatibility mode. Used in conjunction with -e flag. (default: false)
  -e                  Comma separated list of extensions. Extends FUZZ keyword.
  -enc                Encoders for keywords, eg. 'FUZZ:urlencode b64encode'
  -ic                 Ignore wordlist comments (default: false)
  -input-cmd          Command producing the input. --input-num is required when using this input method. Overrides -w.
  -input-num          Number of inputs to test. Used in conjunction with --input-cmd. (default: 100)
  -input-shell        Shell to be used for running command
  -mode               Multi-wordlist operation mode. Available modes: clusterbomb, pitchfork, sniper (default: clusterbomb)
  -request            File containing the raw http request
  -request-proto      Protocol to use along with raw request (default: https)
  -w                  Wordlist file path and (optional) keyword separated by colon. eg. '/path/to/wordlist:KEYWORD'

OUTPUT OPTIONS:
  -debug-log          Write all of the internal logging to the specified file.
  -o                  Write output to file
  -od                 Directory path to store matched results to.
  -of                 Output file format. Available formats: json, ejson, html, md, csv, ecsv (or, 'all' for all formats) (default: json)
  -or                 Don't create the output file if we don't have results (default: false)

EXAMPLE USAGE:
  对路径进行爆破，匹配所有响应，但过滤掉大小为 42 字节的结果，彩色详细输出。
    ffuf -w wordlist.txt -u https://example.org/FUZZ  -mc all -fs 42 -c -v

  对 Host 头进行 Fuzz，只保留 HTTP 200。
    ffuf -w hosts.txt -u https://example.org/  -H "Host: FUZZ" -mc 200

  对 POST JSON 数据爆破，过滤掉包含 "error" 的响应。
    ffuf -w entries.txt -u https://example.org/  -X POST -H "Content-Type: application/json" \
      -d '{"name": "FUZZ", "anotherkey": "anothervalue"}' -fr "error"

  多位置同时爆破，仅保留正文回显 VAL 关键字的结果，彩色输出。
    ffuf -w params.txt:PARAM -w values.txt:VAL -u https://example.org/?PARAM=VAL  -mr "VAL" -c
```

## FUZZ爆破JS案例1

爆破出更多JS结果，再利用其他工具去自动化分析JS中存在的路径以及其他内容。

```shell
ffuf.exe -w httparchive_js_2025_09_27.txt -u http://hvkeokvctxtest319.yihusoft.com/FUZZ -t 200 -c
-w 选取字典文件
-u url
-t 线程
-c 彩色输出
```

![](https://cdn.jsdelivr.net/gh/PWN022/0x00@main/NetSecurity/My_screenshot/14-07.png)

## packer-fuzzer检测案例1

针对使用webpack打包器的网站进行检测。

语法如下：

```shell
python PackerFuzzer.py -u xxxx.com/
```

![](https://cdn.jsdelivr.net/gh/PWN022/0x00@main/NetSecurity/My_screenshot/14-08.png)

检测完毕后，会将结果打包到当前目录reports文件夹下，文件名以检测网站的地址开头，会有htm和doc格式。

打开后可以看到该网站的一些js路径，以及一些可能存在的漏洞详情，方便去检测。

![](https://cdn.jsdelivr.net/gh/PWN022/0x00@main/NetSecurity/My_screenshot/14-09.png)

